if( length(unique(c(RAN$DST[i], RAN$DST[i+1]))) > 1 ) {x <- paste(RAN$dateTime[i]) }
if ( exists('x') & month(RAN$dateTime[i]) < 7 ) { spring <- c(spring, x)} else (fall   <- c(fall,   x) )
rm(x)
}
exists('x')
i
length(unique(c(RAN$DST[i], RAN$DST[i+1]))) > 1
for( i in 1:nrow(RAN) ){
if( length(unique(c(RAN$DST[i], RAN$DST[i+1]))) > 1 ) {x <- paste(RAN$dateTime[i]) }
if ( exists('x') ) {
if (month(RAN$dateTime[i]) < 7 ) { spring <- c(spring, x)} else (fall   <- c(fall,   x) )
rm(x)
}
}
spring
fall
data.frame(spring, fall)
unique(c(RAN$DST[i], RAN$DST[i+1]))
spring <- c()
fall   <- c()
# Loop through all timesteps and note DST changes
for( i in 1:nrow(RAN) ){
if( (length(unique(c(RAN$DST[i], RAN$DST[i+1]))) > 1) & (i < nrow(RAN)) ) {x <- paste(RAN$dateTime[i]) }
if ( exists('x') ) {
if (month(RAN$dateTime[i]) < 7 ) { spring <- c(spring, x)} else (fall   <- c(fall,   x) )
rm(x)
}
}
data.frame(spring, fall)
listDSTHours <- function(minDateTime, maxDateTime, timezone)
require(lubridate)
# Set up time period of interest
ran <- seq(from = as.POSIXct(minDateTime, tz = timezone),
to   = as.POSIXct(maxDateTime, tz = timezone),
by   = "hour")
# Check if DST
RAN <- data.frame(dateTime = ran, DST = dst(ran))
# Set up storage
spring <- c()
fall   <- c()
# Loop through all timesteps and note DST changes
for( i in 1:nrow(RAN) ){
if( (length(unique(c(RAN$DST[i], RAN$DST[i+1]))) > 1) & (i < nrow(RAN)) ) {x <- paste(RAN$dateTime[i]) }
if ( exists('x') ) {
if (month(RAN$dateTime[i]) < 7 ) { spring <- c(spring, x)} else (fall   <- c(fall,   x) )
rm(x)
}
}
# Create final storage:
dstList <- data.frame(spring, fall)
return(dstList)
}
listDSTHours <- function(minDateTime, maxDateTime, timezone){
require(lubridate)
# Set up time period of interest
ran <- seq(from = as.POSIXct(minDateTime, tz = timezone),
to   = as.POSIXct(maxDateTime, tz = timezone),
by   = "hour")
# Check if DST
RAN <- data.frame(dateTime = ran, DST = dst(ran))
# Set up storage
spring <- c()
fall   <- c()
# Loop through all timesteps and note DST changes
for( i in 1:nrow(RAN) ){
if( (length(unique(c(RAN$DST[i], RAN$DST[i+1]))) > 1) & (i < nrow(RAN)) ) {x <- paste(RAN$dateTime[i]) }
if ( exists('x') ) {
if (month(RAN$dateTime[i]) < 7 ) { spring <- c(spring, x)} else (fall   <- c(fall,   x) )
rm(x)
}
}
# Create final storage:
dstList <- data.frame(spring, fall)
return(dstList)
}
min <- '1998-01-01 00:00'
max <- '2012-12-31 00:00'
tz    <- 'US/Eastern'
listDSTHours( minDateTime = , maxDateTime = max, timezone = tz )
listDSTHours( minDateTime = min, maxDateTime = max, timezone = tz )
?overlap
minDateTime
minYear = 1998
maxYear = 2013
timezone = 'US/Eastern'
ran <- seq(from = as.POSIXct(paste0(minYear, "-01-01 00:00"), tz = timezone),
to   = as.POSIXct(paste0(maxYear, "-12-31 00:00"), tz = timezone),
by   = "hour")
ran[1000]
ran[2000]
ran[2200]
ran[2300]
ran[2200:2300]
load('C:/KPONEIL/temporary/dataAfterDates.RData')
?cat
head(data)
rm(list = ls())
# Read libraries
library(RODBC)
# Set base directory
baseDir <- '//IGSAGBEBWS-MJO7/projects/dataIn/environmental/streamTemperature/'
# Define directory containing temperature files
DATA_DIRECTORY <- paste0(baseDir, '/raw/CT/CTDEEP/accessDB/')
# Define directory containing temperature files
WRITE_DIRECTORY <- paste0(baseDir, '/clean/CT/CTDEEP/accessDB/')
# Make the connection
channel <- odbcConnectAccess(paste0(DATA_DIRECTORY, 'water temp 06252014'))
# Query the raw data
data <- sqlQuery( channel , paste ("select *
from `HOBO central data`"), as.is = TRUE)
# Read libraries
library(reshape2)
library(lubridate)
# Set base directory
baseDir <- '//IGSAGBEBWS-MJO7/projects/dataIn/environmental/streamTemperature/'
# Define directory containing temperature files
DATA_DIRECTORY <- '//IGSAGBEBWS-MJO7/projects/dataIn/environmental/streamTemperature/raw'
#DATA_DIRECTORY <- argv[1]
# Define directory containing temperature files
WRITE_DIRECTORY <- '//IGSAGBEBWS-MJO7/projects/dataIn/environmental/streamTemperature/clean'
#WRITE_DIRECTORY <- argv[2]
if (!(file.exists(DATA_DIRECTORY))) {
stop(paste0("Raw base directory does not exist: ", DATA_DIRECTORY))
}
if (!(file.exists(WRITE_DIRECTORY))) {
stop(paste0("Clean base directory does not exist: ", WRITE_DIRECTORY))
}
RELATIVE_DIR <- 'CT/CTDEEP/accessDB'
# Define raw data directory containing xls temperature files
DATA_DIRECTORY <- file.path(DATA_DIRECTORY, RELATIVE_DIR)
# Define clean data directory for writing metadata files
WRITE_DIRECTORY <- file.path(WRITE_DIRECTORY, RELATIVE_DIR)
# Define clean data directory for writing csv temperature files
WRITE_FILES_DIRECTORY <- file.path(WRITE_DIRECTORY, 'files')
if (!(file.exists(WRITE_DIRECTORY))) {
cat(paste0('Creating clean data folder: ', WRITE_DIRECTORY, '\n'))
dir.create(WRITE_DIRECTORY, recursive=TRUE)
dir.create(WRITE_FILES_DIRECTORY, recursive=TRUE)
}
cat(paste0(rep('=', 80), collapse=''), '\n')
cat(paste0("Raw data folder: ", DATA_DIRECTORY), '\n')
cat(paste0("Clean data folder: ", WRITE_DIRECTORY), '\n')
dataPath <- file.path(DATA_DIRECTORY, 'water temp 06252014.csv')
cat('Reading raw data file: ', dataPath, '\n')
data <- read.table(file = dataPath, sep = ",", stringsAsFactors = F)
load('C:/KPONEIL/temporary/dataBeforeTimeCorr.RData')
#############
# FUNCTIONS #
#############
# This function returns the date and hour of all daylight savings times in a given time period.
#   The result is a dataframe of spring and fall DST datetimes in UTC format of the original
listDSTHours <- function(minYear, maxYear, timezone){
require(lubridate)
# Set up time period of interest
ran <- seq(from = as.POSIXct(paste0(minYear, "-01-01 00:00"), tz = timezone),
to   = as.POSIXct(paste0(maxYear, "-12-31 00:00"), tz = timezone),
by   = "hour")
# Check if DST
RAN <- data.frame(dateTime = ran, DST = dst(ran))
# Set up storage
spring <- c()
fall   <- c()
# Loop through all timesteps and note DST changes
for( i in 1:nrow(RAN) ){
if( (length(unique(c(RAN$DST[i], RAN$DST[i+1]))) > 1) & (i < nrow(RAN)) ) {x <- paste(RAN$dateTime[i]) }
if ( exists('x') ) {
if (month(RAN$dateTime[i]) < 7 ) { spring <- c(spring, x)} else (fall   <- c(fall,   x) )
rm(x)
}
}
# Create output storage
dstList <- data.frame( spring = ymd_hms(spring, tz = 'UTC'),  fall = ymd_hms(fall, tz = 'UTC') )
return(dstList)
}
# Process datetime data was entered:
#data$enteredDate <- ymd_hms(data$entered.date, tz = 'US/Eastern')
# Convert to POSIXct format
# Converting directly to US/Eastern POSIXct fails because at least some of the raw records do not account for DST
#data$dateTimeET <- force_tz(data$dateTimeUTC, tz = 'US/Eastern')
# Generate DST datetime list
DSTs <- listDSTHours( minYear = year(min(data$dateTimeUTC, na.rm = T)),
maxYear = year(max(data$dateTimeUTC, na.rm = T)),
timezone = 'US/Eastern')
# Note: DSTs are marked @ 1am to avoid confusion when times jump. (i.e. 2am doesn't technically exist on spring DST change)
#i = 407
siteDeps <- unique(data$siteDeploy)
needToFixSpr <- c()
needToFixFall <- c()
library(lubridate)
library(reshape2)
for( i in seq_along(siteDeps) ){
# Index a site and deployment:
temp <- data[which(data$siteDeploy == siteDeps[i]),]
# Add a column for corrected datetime
temp$corr <- temp$dateTimeUTC
# Determine which DST datetimes are covered by the range
d <- DSTs[ apply(DSTs, c(1,2), function(x){(x >= range(temp$dateTimeUTC)[1]) && (x <= range(temp$dateTimeUTC)[2])}) ]
# Because the POSIXct class gets dropped. Make it a list to preserve the class.
d1 <- list(as.POSIXct(d, tz = 'UTC'))
intDSTs <- d1[[1]][order(d1[[1]])]
# Check for evidence of DST correction
# ------------------------------------
# Loop through DSTs and see if changes need to be made
for ( j in seq_along(intDSTs) ){
# Index current DSTs
curDST <- intDSTs[j]
# Only going to change that which comes after the current DST ( We will loop through them chronologically)
aft <- which(temp$corr >= curDST + dhours(1))
# Spring
if( month(curDST) < 7 ){
# If there are records in the time period that should not exist, then it will need to be changed
beg <- curDST + dhours(1); end <- curDST + dhours(2)
# Which records fall in the period that "doesn't exist"
indicateSpr <- temp$dateTimeUTC[which(unlist(lapply(temp$dateTimeUTC, function(x){(x >= beg) && (x < end)} )) == TRUE)]
# If there are some entries in this hour it is indicative that a change must be made and the record after it gets bumped up an hour
if( length(indicateSpr) > 0 ) { temp$corr[aft] <- temp$corr[aft] + dhours(1)}
#Check
needToFixSpr <- c(needToFixSpr, length(indicateSpr) > 0)
}
# Fall
if( month(curDST) >= 7 ){
# If there are not duplicate records in the time period that gets duplicated then changes should be made.
beg <- curDST; end <- curDST + dhours(1)
# Which records fall within the period of concern
indicateFall <- temp$dateTimeUTC[which(unlist(lapply(temp$dateTimeUTC, function(x){(x >= beg) && (x < end)} )) == TRUE)]
# If there are not duplicates, then make the change to account for DST
if( all(duplicated(indicateFall) == FALSE) ) { temp$corr[aft] <- temp$corr[aft] - dhours(1) }
#Check
needToFixFall <- c(needToFixFall, all(duplicated(indicateFall) == FALSE))
}
# Remove to prevent carryover
rm(beg, end, curDST, aft)
if(exists('indicateSpr') )  {rm(indicateSpr)}
if(exists('indicateFall') ) {rm(indicateFall)}
}
temp$corrET <- force_tz(temp$corr, tz = 'US/Eastern')
if ( i == 1) { fin <- temp } else( fin <- rbind( fin, temp ) )
rm(temp)
print(i)
}
dim(fin)
dim(data)
data <- fin
save(data, file = 'C:/KPONEIL/temporary/dataAfterTimeCorr.RData')
head(fin)
head(allMeta)
?gsub
names(data) !%in% removeColumns
removeColumns <- c('corr', 'dateTime')
!names(data) %in% removeColumns
!names(data) %in% removeColumns)
!names(data) %in% removeColumns
head(data)
names(data)[which(names(data) = 'corrET')]
names(data)[which(names(data) == 'corrET')] <- 'dateTimeEST'
head(data)
removeColumns <- c('siteDeploy', 'corr', 'dateTime', 'dateTimeUTC')
data <- data[,!names(data) %in% removeColumns]
names(data) <- gsub('.', ' ', names(data))
head(data)
names(data)
load(file = 'C:/KPONEIL/temporary/dataAfterTimeCorr.RData')
head(data)
names(data)[which(names(data) == 'corrET')]
names(data)[which(names(data) == 'corrET')] <- 'dateTimeEST'
gsub('.', ' ', names(data))
gsub('.', '-', names(data))
names(data)
gsub('.', '-', names(data), fixed = T)
gsub('.', ' ', names(data), fixed = T)
removeColumns <- c('siteDeploy', 'corr', 'dateTime', 'dateTimeUTC')
data <- data[,!names(data) %in% removeColumns]
names(data) <- gsub('.', ' ', names(data), fixed = TRUE)
head(data)
setwd(WRITE_FILES_DIRECTORY)
WRITE_FILES_DIRECTORY
library(xlsx)
update.packages(checkBuilt=TRUE, ask=FALSE)
library(devtools)
library(maptools)
library(maptools)
library(devtools)
find_rtools()
find_rtools()
?find_rtools()
find_rtools
find_rtools()
library(devtools)
library(ncdf4)
library(maptools)
proj4.NHD  <- "+proj=longlat +ellps=GRS80 +datum=NAD83 +no_defs"
catchments <- readShapePoly ( "C:/KPONEIL/gis/nhdPlusV2/NENY_NHDCatchment.shp", proj4string=CRS(proj4.NHD))
NCDF <- nc_open('C:/KPONEIL/temporary/dayl_1980.nc4')    #netcdf
print(NCDF)
variables <- "dayl"
catchmentShapefile <- catchments
start1 = c(1,1)
latcount <- c(NCDF$var$lat$varsize[1], NCDF$var$lat$varsize[2])
loncount <- c(NCDF$var$lon$varsize[1], NCDF$var$lon$varsize[2])
YDcount  <- NCDF$var$yearday$varsize
# Read in variables
lat = ncvar_get ( nc=NCDF, varid="lat", start = start1, count = latcount )
lon = ncvar_get ( nc=NCDF, varid="lon", start = start1, count = loncount )
dOY = ncvar_get ( nc=NCDF, varid="yearday",             start = 1,      count = YDcount  )
# Correction for Daymet doy which starts at 0.
dOY <- dOY + 1
# Join coordinate lists
masterCoords <- cbind( as.vector(lon), as.vector(lat))
colnames(masterCoords) <- c("Longitude", "Latitude")
masterCoordsMatrix <- masterCoords
masterCoords <- as.data.frame(masterCoords)
fids <- catchments@data$FEATUREID
i = 1
print(paste0(round(i/length(fids), digits = 3)*100, '% done.'))
catchmentShape <- as(catchmentShapefile[catchmentShapefile$FEATUREID %in% fids[i],], "SpatialPolygons")
# Determine which points fall inside catchment
a <- SpatialPoints(masterCoords, proj4string=CRS(proj4.NHD))
inside <- as.data.frame(a[!is.na(over(a, catchmentShape)),])
inside
EXT <- extent(catchmentShape)
library(raster)
EXT <- extent(catchmentShape)
EXT
EXT@xmin
tempCoords <- masterCoords[which(masterCoords$Latitude >= EXT@ymin & masterCoords$Latitude <= EXT@ymax & masterCoords$Longitude >= EXT@xmin & masterCoords$Longitude <= EXT@xmax),]
tempCoords
b <- SpatialPoints(masterCoords, proj4string=CRS(proj4.NHD))
inside <- as.data.frame(a[!is.na(over(a, catchmentShape)),])
inside <- as.data.frame(b[!is.na(over(b, catchmentShape)),])
inside
inside <- as.data.frame(a[!is.na(over(a, catchmentShape)),])
names(inside) <- c('Longitude', 'Latitude')
inside
for ( m in 1:nrow(inside) ){
position <- which(lon == inside$Longitude[m] & lat == inside$Latitude[m], arr.in = TRUE)
start2 = c(as.numeric(position[,1]), as.numeric(position[,2]), 1)
varcount = c(1, 1, NCDF$var$yearday$varsize)
var = ncvar_get ( nc=NCDF, varid= paste0(variables[j]), start = start2, count = varcount )
nc_close(NCDF)
if (m == 1) {tempVar <- data.frame(dOY, var)} else(tempVar <- cbind(tempVar, var))
}
j = 1
for ( m in 1:nrow(inside) ){
position <- which(lon == inside$Longitude[m] & lat == inside$Latitude[m], arr.in = TRUE)
start2 = c(as.numeric(position[,1]), as.numeric(position[,2]), 1)
varcount = c(1, 1, NCDF$var$yearday$varsize)
var = ncvar_get ( nc=NCDF, varid= paste0(variables[j]), start = start2, count = varcount )
nc_close(NCDF)
if (m == 1) {tempVar <- data.frame(dOY, var)} else(tempVar <- cbind(tempVar, var))
}
print(NCDF)
NCDF <- nc_open('C:/KPONEIL/temporary/dayl_1980.nc4')    #netcdf
print(NCDF)
m
for ( m in 1:nrow(inside) ){
position <- which(lon == inside$Longitude[m] & lat == inside$Latitude[m], arr.in = TRUE)
start2 = c(as.numeric(position[,1]), as.numeric(position[,2]), 1)
varcount = c(1, 1, NCDF$var$yearday$varsize)
var = ncvar_get ( nc=NCDF, varid= paste0(variables[j]), start = start2, count = varcount )
if (m == 1) {tempVar <- data.frame(dOY, var)} else(tempVar <- cbind(tempVar, var))
}
tempVar
tempVar[,-c('dOY')]
tempVar[,"var"]
tempVar[,c("var")]
ifelse( ncol(tempVar) > 2, R <- rowMeans(tempVar[,-1], na.rm = TRUE, dims = 1),  R <- tempVar[,-1] )
head(R)
tempVar <- data.frame(tempVar$dOY, R)
head(tempVar)
variable = "dayl"
tempVar <- data.frame(dOY = tempVar$dOY, paste(variable) = R)
for ( m in 1:nrow(inside) ){
position <- which(lon == inside$Longitude[m] & lat == inside$Latitude[m], arr.in = TRUE)
start2 = c(as.numeric(position[,1]), as.numeric(position[,2]), 1)
varcount = c(1, 1, NCDF$var$yearday$varsize)
var = ncvar_get ( nc=NCDF, varid= paste0(variables[j]), start = start2, count = varcount )
if (m == 1) {tempVar <- data.frame(dOY, var)} else(tempVar <- cbind(tempVar, var))
}
ifelse( ncol(tempVar) > 2, R <- rowMeans(tempVar[,-1], na.rm = TRUE, dims = 1),  R <- tempVar[,-1] )
varMeans <- data.frame(dOY = tempVar$dOY, paste(variable) = R)
variable
paste(variable)
R
varMeans <- data.frame(dOY = tempVar$dOY)
head(varMeans)
varMeans[,paste(fid)] <- R
varMeans[,paste(fid[i])] <- R
varMeans[,paste(fids[i])] <- R
head(varMeans)
CRS(catchments)
variable <- 'tmin'
year <- '2012'
address <- paste0('ftp://daac.ornl.gov/data/daymet/Daymet_mosaics/data/', variable, '_', year, '.nc4')
beg <- proc.time()[3]
download.file(url = address, destfile = paste0('C:/Users/koneil/Downloads', variable,'_', year, '.nc') )
end <- proc.time()[3]
runTime <- (end - beg)/3600
14/32
12000*5
/3600
60000/3600
install.packages("C:/Users/koneil/Downloads/khufkens-daymetr-5185bb94b4a5.zip", repos = NULL)
install.packages("C:/Users/koneil/Downloads/khufkens-daymetr-5185bb94b4a5/DaymetR.tar.gz", repos = NULL, type = "source")
library(DaymetR)
get.daymet(site="Oak Ridge National Laboratories",lat=36.0133,lon=-84.2625,start_yr=1980,end_yr=2010,internal=TRUE)
?DaymetR
get.Daymet(site="Oak Ridge National Laboratories",lat=36.0133,lon=-84.2625,start_yr=1980,end_yr=2010,internal=TRUE)
ls()
Oak Ridge National Laboratories
"Oak Ridge National Laboratories"
?get.Daymet
get.Daymet(site="test1",lat=36.0133,lon=-84.2625,start_yr=1980,end_yr=2010,internal=TRUE)
test1
head(test1)
str(test1)
library(maptools)
catchments <- readShapePoly ( "C:/KPONEIL/gis/nhdPlusV2/stateCatchments/MA_Catchment.shp", proj4string=CRS(proj4.NHD))
proj4.NHD  <- "+proj=longlat +ellps=GRS80 +datum=NAD83 +no_defs"
catchments <- readShapePoly ( "C:/KPONEIL/gis/nhdPlusV2/stateCatchments/MA_Catchment.shp", proj4string=CRS(proj4.NHD))
centroids <- coordinates(catchments)
head(centroids)
centroids <- data.frame(catchments@data$FEATUREID, coordinates(catchments))
head(centroids)
centroids <- data.frame(FEATUREID = catchments@data$FEATUREID, coordinates(catchments))
head(centroids)
for( i in 1:100){
get.Daymet(site=paste(centroids$FEATUREID[i]), lat=centroids[,2], lon=centroids[,1], start_yr=1980, end_yr=2013,internal=TRUE)
print(i)
}
for( i in 1:100){
get.Daymet(site=paste(centroids$FEATUREID[i]), lat=centroids[,3], lon=centroids[,2], start_yr=1980, end_yr=2013,internal=TRUE)
print(i)
}
warnings
warnings()
for( i in 1:100){
get.Daymet(site=paste(centroids$FEATUREID[i]), lat=centroids[i,3], lon=centroids[i,2], start_yr=1980, end_yr=2013,internal=TRUE)
print(i)
}
beg <- proc.time()[3]
for( i in 1:100){
get.Daymet(site=paste(centroids$FEATUREID[i]), lat=centroids[i,3], lon=centroids[i,2], start_yr=1980, end_yr=2013,internal=TRUE)
print(i)
}
end <- proc.time()[3]
runTime <- (end - beg)/3600
beg <- proc.time()[3]
for( i in 1:100){
get.Daymet(site=paste(centroids$FEATUREID[i]), lat=centroids[i,3], lon=centroids[i,2], start_yr=1980, end_yr=2013,internal=TRUE)
print(i)
}
end <- proc.time()[3]
setwd('C:/KPONEIL/GitHub/personal/DATAing/daymet')
my_db <- src_sqlite("test.sqlite3", create = T)
library(dplyr)
library(RSQLite)
my_db <- src_sqlite("test.sqlite3", create = T)
my_db
tbl(my_db, "hflights")
select(hflights_sqlite, Year:DayofMonth, DepDelay, ArrDelay)
hflights_sqlite
hflights_sqlite <- copy_to(my_db, hflights, temporary = FALSE, indexes = list(
c("Year", "Month", "DayofMonth"), "UniqueCarrier", "TailNum"))
data("hflights", package = "hflights")
select(hflights_sqlite, Year:DayofMonth, DepDelay, ArrDelay)
hflights_sqlite <- copy_to(my_db, hflights, temporary = FALSE, indexes = list(
c("Year", "Month", "DayofMonth"), "UniqueCarrier", "TailNum"))
select(hflights_sqlite, Year:DayofMonth, DepDelay, ArrDelay)
?select
select(hflights, Year:DayofMonth, DepDelay, ArrDelay)
?tbl
hflights_sqlite <- tbl(hflights_sqlite(), "hflights")
select(hflights_sqlite, Year:DayofMonth, DepDelay, ArrDelay)
filter(hflights_sqlite, depDelay > 240)
arrange(hflights_sqlite, Year, Month, DayofMonth)
?mutate
test <- data.frame(x = c(1,2,3), y = c(4,5,6), z = c(7,8,9))
test
mutate(test, a = x+y)
arrange(test, a)
arrange(test, "a")
filter(test, x = 1)
filter(test, x > 2)
arrange(test, x)
test <- mutate(test, a = x+y)
arrange(test, x)
filter(test, x > 2)
summarise(x,mean)
summarise(test, mean)
summarise(test, fun = mean)
summarise(test)
test
summarise(test, x)
group_by(test, x)
group_by(test, y)
group_by(test, x,y)
select(hflights_sqlite, Year:DayofMonth, DepDelay, ArrDelay)
filter(hflights_sqlite, depDelay > 240)
arrange(hflights_sqlite, Year, Month, DayofMonth)
mutate(hflights_sqlite, speed = AirTime / Distance)
c1 <- filter(hflights_sqlite, DepDelay > 0)
c2 <- select(c1, Year, Month, DayofMonth, UniqueCarrier, DepDelay, AirTime, Distance)
c3 <- mutate(c2, Speed = Distance / AirTime * 60)
c4 <- arrange(c3, Year, Month, DayofMonth, UniqueCarrier)
c4
collect(c4)
t <- collect(c4)
t
str(t)
t[1:100,]
collect(c4)
c4$query
